{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP3670/6670 Week 5: Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll be covering two main clustering algorithms: agglomerative hierarchical clustering & k-means clustering. We'll do this by implementing both algorithms, and using them to cluster some toy datasets.\n",
    "\n",
    "### Before you start, you should understand:\n",
    "* Goals of unsupervised learning\n",
    "* Programming with numpy & matplotlib\n",
    "\n",
    "If you are not comfortable with any of these topics, you should discuss with your tutors now.\n",
    "\n",
    "### After this lab, you should be comfortable with the: \n",
    "* goals of clustering\n",
    "* K-means clustering algorithm\n",
    "* agglomerative clustering algorithm\n",
    "* differences and tradeoffs between both algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set-up code\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter magic for inline figures\n",
    "%matplotlib inline \n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'Accent' # Changing the default colour-scheme for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is clustering?\n",
    "\n",
    "Clustering is a form of *unsupervised learning* where we aim to group our data into disjoint sets of similar elements called \"clusters\". We can descibe this more formally as follows:\n",
    "\n",
    "Given a data set $D = \\{\\textbf{x}_1, \\ldots \\textbf{x}_N \\}$,\n",
    "we would like to classify the data set into $K$ many clusters,\n",
    "grouping the points into the clusters based on similarity. \n",
    "\n",
    "For each point $\\textbf{x}_n$, we introduce the binary indicator \n",
    "$r_{nk} \\in \\{0,1\\}$, which indicates to which cluster $\\textbf{x}_n$ belongs.\n",
    "\n",
    "$$\n",
    "r_{nk} = \\begin{cases}\n",
    "1 & \\textbf{x}_n \\text{ is assigned to cluster }k \\\\\n",
    "0 & \\text{otherwise} \\end{cases}\t\n",
    "$$\n",
    "\n",
    "We also introduce a collection of vectors $\\{ \\boldsymbol{\\mu}_1, \\ldots \\boldsymbol{\\mu}_K \\}$,\n",
    "where $\\boldsymbol{\\mu}_k$ is a vector that represents cluster $k$.\n",
    "\n",
    "TODO keep?\n",
    "The loss function (which measures the squared distance of \n",
    "how far each data point $\\boldsymbol{x}_n$ is away from the \n",
    "representative vector $\\boldsymbol{\\mu}_k$ of its cluster) is given by\n",
    "\n",
    "$$\n",
    "L = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} ||\\textbf{x}_n - \\boldsymbol{\\mu}_k||_2^2\n",
    "$$\n",
    "\n",
    "In this lab, we will explore two algorithms for solving this problem: k-means clustering and agglomerative clustering.\n",
    "\n",
    "To do this, we'll start out by generating a dataset to perform clustering on. Sklearn has a handy function called `make_blobs` which draws datapoints from several Gaussian distributions to form a dataset with multiple different \"groups\" of data. Our goal will be to use clustering techniques to figure out which Gaussian each datapoint was drawn from. We can then use the `labels` return value of `make_blobs` as our ground truth to compare our results against. \n",
    "\n",
    "**Exercise:**\n",
    "Complete the `plot_2d_clusters` function below. It should take the 2-d data and labels generated from `make_blobs` and generate a scatterplot with each label given a unique colour. \n",
    "*Hint:* use the `c` parameter of `plt.scatter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_2d_clusters(data, labels):\n",
    "    \"\"\"Plot a set of 2-d datapoints (data) with colours according to the label.\"\"\"\n",
    "    \n",
    "    raise NotImplementedError # Remove this to run the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clustering Visualisation Test Code\n",
    "\n",
    "from sklearn.datasets import make_blobs # Function for generating Gaussian clusters\n",
    "\n",
    "centers = 5 # Number of clusters.\n",
    "n_samples = 300 # Number of datapoints to generate\n",
    "\n",
    "random_state = 42 # Random seed\n",
    "data, labels = make_blobs(centers=centers, n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "plt.figure(figsize=(6, 12))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"make_blobs 5-cluster example\")\n",
    "plot_2d_clusters(data, labels)\n",
    "\n",
    "# Generate the dataset we'll use for the rest of this lab.\n",
    "centers = [(-2, -2), (2, 2)] # We can also specify centroid coordinates\n",
    "data, labels = make_blobs(centers=centers, n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"2-cluster dataset\")\n",
    "plot_2d_clusters(data, labels)\n",
    "\n",
    "plt.plot()\n",
    "None # Don't display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first clustering algorithm we'll discuss is **agglomerative clustering**. This algorithm is part of a family of algorithms called **Hierarchical clustering**, where we seek to build up a hierarchy of clusters, based on how similar points are to each other.\n",
    "These algorithms fall into two categories: **agglomerative**, where each data point is assigned it's own\n",
    "cluster, and then similar clusters are then merged together, or **divisive**,\n",
    "where all the data points are grouped together in a single cluster, and split into\n",
    "smaller clusters recursively. \n",
    "\n",
    "For this lab, we'll focus on a specific type of agglomerative clustering called **centroid linkage clustering**, described below.\n",
    "\n",
    "Let $\\mathcal{C}$ be a set of clusters, each cluster being a set of datapoints.\n",
    "For every cluster $C \\in \\mathcal{C}$, we can compute the centroid ${\\mu}_C$ as the representative of that cluster:\n",
    "$$\n",
    "{\\mu}_C := \\frac{1}{|C|} \\sum_{x \\in C} x\n",
    "$$\n",
    "Now for any two clusters we define the distance between two clusters as the\n",
    "distance between the centroids of each cluster:\n",
    "$$\n",
    "d(C,D) := ||{\\mu}_C - {\\mu}_D||_2\n",
    "$$\n",
    "\n",
    "Note that in practise we don't need to recompute the centroid each time if the cluser is unchanged. Instead, we can store the centroid with the cluster for efficiency purposes.\n",
    "\n",
    "The agglomerative clustering algorithm thus is the following:\n",
    "\n",
    "1. **Initalise:** Set $\\mathcal{C} = \\{ \\{\\textbf{x}_1\\} , \\{\\textbf{x}_2\\}, \\ldots, \\{\\textbf{x}_n\\} \\}$ (assign each point it's own cluster.),\n",
    "    and for all $C \\in \\mathcal{C},$ set $\\mu_C$ to be the single point $\\textbf{x}_i$ in that cluster. \n",
    "    \n",
    "2. **Merge:** Find the clusters $C$ and $D$ such that $||\\mu_C - \\mu_D||_2$ \n",
    "    is minimized. That is, find the clusters that are closest to each other.\n",
    "    \n",
    "3. **Update:**\n",
    "    Remove both $C$ and $D$ from $\\mathcal{C}$, and add $C \\cup D$ to \n",
    "    $\\mathcal{C}$. Set \n",
    "    $$\n",
    "    \\mu_{C \\cup D} = \\frac{1}{|C \\cup D|} \\sum_{\\textbf{x} \\in C \\cup D} \\textbf{x}\n",
    "    $$\n",
    "    If the number of clusters $|\\mathcal{C}|$ is now equal to $K$, terminate. \n",
    "    Else, goto **Merge**.\n",
    "    \n",
    "    \n",
    "**Exercise:** Implement `agglomerative_clustering` below for 2-dimensional data, assuming k=2 (2 clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglomerative_clustering(data):\n",
    "    \"\"\"Cluster a dataset using the agglomerative clustering algorithm.\n",
    "    \n",
    "    Note that you need to return an array of integers (either 0 or 1) for each datapoint representing which\n",
    "    of the two clusters they belong to. Remember that labels are arbritrary (i.e. you can swap all 0s and 1s).\"\"\"\n",
    "\n",
    "    raise NotImplementedError # Remove this to run the code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering Test Code\n",
    "pred_labels = agglomerative_clustering(data)\n",
    " \n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Agglomerative Clustering Predicted Labels\")\n",
    "plot_2d_clusters(data, pred_labels)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"True Labels\")\n",
    "plot_2d_clusters(data, labels)\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "None # Don't display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second clustering algorithm we focus on in this course is **K-Means clustering**. In comparison to the agglomerative clustering algorithm, K-Means clustering starts by making a random \"guess\" at the location of centroids, then iteratively optimises the loss function (defined below) until reaching a local minimum.  \n",
    "\n",
    "\n",
    "For each point $\\textbf{x}_n$, we introduce the binary indicator \n",
    "$r_{nk} \\in \\{0,1\\}$, which indicates to which cluster $\\textbf{x}_n$ belongs.\n",
    "\n",
    "$$\n",
    "r_{nk} = \\begin{cases}\n",
    "1 & \\textbf{x}_n \\text{ is assigned to cluster }k \\\\\n",
    "0 & \\text{otherwise} \\end{cases}\t\n",
    "$$\n",
    "\n",
    "We introduce a collection of vectors $\\{ \\boldsymbol{\\mu}_1, \\ldots \\boldsymbol{\\mu}_K \\}$,\n",
    "where $\\boldsymbol{\\mu}_k$ is a vector that represents cluster $k$.\n",
    "\n",
    "The loss function (which measures the squared distance of \n",
    "how far each data point $\\boldsymbol{x}_n$ is away from the \n",
    "representative vector $\\boldsymbol{\\mu}_k$ of its cluster) is given by\n",
    "\n",
    "$$\n",
    "L = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} ||\\textbf{x}_n - \\boldsymbol{\\mu}_k||_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We can implement the `k_means_clustering` function to cluster 2-dimensional data using scikit-learn, assuming k=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "def k_means_clustering(data):\n",
    "    \"\"\"Cluster a dataset using the K-means clustering algorithm.\n",
    "    \n",
    "    Note that you need to return an array of integers (either 0 or 1) for each datapoint representing which\n",
    "    of the two clusters they belong to. Remember that labels are arbritrary (i.e. you can swap all 0s and 1s).\"\"\"\n",
    "    kmeans = KMeans(n_clusters=2).fit(data) \n",
    "    return kmeans.labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# K-Means Test Code\n",
    "\n",
    "pred_labels = k_means_clustering(data)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"K-Means Clustering Predicted Labels\")\n",
    "plot_2d_clusters(data, pred_labels)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"True Labels\")\n",
    "plot_2d_clusters(data, labels)\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "None # Don't display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative vs K-Means Clustering\n",
    "\n",
    "We've now seen two approaches to doing clustering. As you should see from the tests, the two algorithms behave approximately the same with clusters that are far enough appart. We'll compare them on a much trickier dataset: two clusters with a large amount of overlap. We'll use four metrics for this comparison:\n",
    "\n",
    "1. Visual Inspection\n",
    "2. Accuracy\n",
    "3. Loss\n",
    "4. Runtime\n",
    "\n",
    "The code necessary for visual inspection and runtime analysis is provided for you below.\n",
    "\n",
    "**Exercise:** Implement the `accuracy` and `loss` functions below. `Accuracy` is defined as $\\frac{\\textrm{\\# of correct predictions}}{\\textrm{\\# of predictions}}$, and `loss` is defined as the loss function for k-means clustering (above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_labels, true_labels):\n",
    "    \"\"\"Compute the accuracy of the predicated labels vs the true labels.\n",
    "    \n",
    "    Remember that the labels may be inverted (i.e. the original 0s may now be the 1s).\n",
    "    You should use whichever set of labellings have the best accuracy. \"\"\"\n",
    "    \n",
    "    raise NotImplementedError # Remove this to run the code!\n",
    "    \n",
    "def loss(data, labels):\n",
    "    \"\"\"Compute the loss of the dataset according to the predicted labels.\n",
    "    \n",
    "    Hint: it may be useful to use a helper function to compute the loss for each centroid.\"\"\"\n",
    "    \n",
    "    raise NotImplementedError # Remove this to run the code!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time\n",
    "\n",
    "# Clustering Comparison Test Code\n",
    "centers = [(2, 2), (4,4)]\n",
    "data, labels = make_blobs(centers=centers, n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "before_time = process_time()\n",
    "agglomerative_labels = agglomerative_clustering(data)\n",
    "duration = process_time() - before_time\n",
    "print(f\"Agglomerative Clustering Runtime: {duration * 1000:.1f}ms\")\n",
    "\n",
    "before_time = process_time()\n",
    "k_means_labels = k_means_clustering(data)\n",
    "duration = process_time() - before_time\n",
    "print(f\"K-Means Clustering Runtime: {duration * 1000:.1f}ms\")\n",
    "print()\n",
    "\n",
    "#Print statistics\n",
    "print(f\"Agglomerative Clustering Accuracy: {accuracy(agglomerative_labels, labels) * 100:.2f}%\")\n",
    "print(f\"K-Means Clustering Accuracy: {accuracy(k_means_labels, labels) * 100:.2f}%\")\n",
    "print()\n",
    "print(f\"Average Loss (True): {loss(data, labels)/n_samples:.2f}\")\n",
    "print(f\"Average Loss (Agglomerative): {loss(data, agglomerative_labels)/n_samples:.2f}\")\n",
    "print(f\"Average Loss (K_Means): {loss(data, k_means_labels)/n_samples:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title(\"Agglomerative Clustering Predicted Labels\")\n",
    "plot_2d_clusters(data, agglomerative_labels)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"True Labels\")\n",
    "plot_2d_clusters(data, labels)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"K-Means Clustering Predicted Labels\")\n",
    "plot_2d_clusters(data, k_means_labels)\n",
    "\n",
    "plt.plot()\n",
    "None # Don't display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What do you notice in terms of the performances of k-means clustering vs agglomerative clustering? Discuss the results with your neighbours, and try to explain why each observation may occur:\n",
    "\n",
    "* Runtime: \n",
    "* Accuracy & Loss: \n",
    "* Visual Observations: \n",
    "\n",
    "**Exercise:** For k-means clustering, what factors affects the run time? \n",
    "\n",
    "Answer: \n",
    "\n",
    "**Exercise:** You should notice that the average loss for K_Means is less than the average loss for the true labels. Does this make sense? What causes it?\n",
    "\n",
    "Answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

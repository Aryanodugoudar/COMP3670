{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMP3670/6670 Tutorial Week 9 - Linear regression and GMMs**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression and gradient descent are pillars in machine learning. The first part of this tutorial to go over the lecture slides in linear regression and gradient descent.  \n",
    "\n",
    "1. Ensure you understand (stochastic) gradient desent.\n",
    "2. Ensure you could can derive the gradient of the least squares objective. \n",
    "\n",
    "Once that's all done, revisit GMMs and the EM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----------\n",
    "\n",
    "   **TASK 1: linear regression with gradient descent.** \n",
    "   \n",
    "   1. Randomly generate a matrix $X \\in \\mathbb{R}^{N \\times D}$, where each row of $X$ is a training example.\n",
    "   2. Choose a vector $t \\in \\mathbb{R}^{D \\times 1}$.\n",
    "   3. Generate $Y$ by $Xt = Y$.\n",
    "   4. Then generate a random matrix $\\theta \\in \\mathbb{R}^{D \\times 1}$.\n",
    "   5. Implement gradient descent to find the maximum likelihood estimate $\\theta$.\n",
    "   6. Check your gradient descent algorithm correctly approximated $t$. Talk to your classmates and tutor to make sure if you're unsure.\n",
    "   7. Verify your answer with the closed form solution employing the Moore-Penrose inverse.\n",
    "   \n",
    "Note that in the above we're essentially pretending we don't know $t$. Obviously, if we have $t$, linear regression with gradient descent would be unnecessary, but the point is to help you understand what gradient descent is doing.\n",
    "\n",
    "Also note: we should use the squared loss function, computed as the square of the difference between the predicted function values and the observed function values (or ground truth). $D$ and $N$ can be any number you like, but be reasonable.\n",
    "\n",
    "\n",
    "-----------\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "**Task 2:** We investigate various factors in linear regression \n",
    "\n",
    "1. noise. When collecting real-world data, it is common that there would be measurement noise included. Adding noise to your generated data and see how this would influence the parameter estimation. You can add noise by settting $Y=Xt+\\pmb\\epsilon$ where $\\epsilon_n \\sim \\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "\n",
    "2. sample amount. Try to change the number of data points in your training set. That is changing the $N$ for $X \\in \\mathbb{R}^{N \\times D}$ and comparing the final loss fixing training epoch and learning rate. Now try to set N to be very large, how is the training time? What can we do?\n",
    "\n",
    "3. learning rate. How would the learning rate influence the convergence of the optimization process?\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "**Task 3:** GMMs - responsibility computation using logsumexp\n",
    "\n",
    "The E-step of parameter learning for GMMs computes the responsibility: $r_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n; \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n; \\mu_j, \\Sigma_j)}$. Instead of doing the computation using the Gaussian densities, using the log densities and *logsumexp* is more numerically stable. In code, the responsibility can be computed as $r_{nk} = \\exp \\left( \\log \\pi_k + \\log \\mathcal{N}(x_n; \\mu_k, \\Sigma_k)  - \\text{logsumexp}_j \\left[ \\log \\pi_j + \\log \\mathcal{N}(x_n; \\mu_j, \\Sigma_j) \\right] \\right)$.\n",
    "\n",
    "1. Verify that the two expressions above are mathematically identical.\n",
    "2. We have implemented the first version below (first equation). Implement the version that uses logsumexp (second equation) and compare the results.\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_responsibility_v1(x, pis, mus, Sigmas):\n",
    "    K = len(pis)\n",
    "    pdfs = np.array([multivariate_normal.pdf(x[None, :], mean=mus[k, :], cov=Sigmas[k, :, :]) for k in range(K)])\n",
    "    pipdfs = pis * pdfs\n",
    "    denom = np.sum(pipdfs)\n",
    "    res = pipdfs / denom\n",
    "    return res\n",
    "\n",
    "# you can use scipy logsumexp function\n",
    "def compute_responsibility_v2(x, pis, mus, Sigmas):\n",
    "    # YOUR CODE HERE\n",
    "    return 0\n",
    "\n",
    "# we consider two GMMs, each with 3 components, same weights and same means\n",
    "pis = np.array([0.4, 0.5, 0.1])\n",
    "mu_1 = np.array([-1, 1])\n",
    "mu_2 = np.array([1, 1])\n",
    "mu_3 = np.array([-2, 2])\n",
    "mus = np.array([mu_1, mu_2, mu_3])\n",
    "# and different covariances\n",
    "Sigmas_1 = np.array([np.eye(2) for k in range(3)])\n",
    "Sigmas_2 = np.array([0.05*np.eye(2) for k in range(3)])\n",
    "\n",
    "# we have two points that we want to compute the responsibilities\n",
    "x_1 = np.array([-1, 1])\n",
    "x_2 = np.array([-10, 10])\n",
    "\n",
    "# let's look at the first data point and Sigma_1\n",
    "r1 = compute_responsibility_v1(x_1, pis, mus, Sigmas_1)\n",
    "r2 = compute_responsibility_v2(x_1, pis, mus, Sigmas_1)\n",
    "print(r1, r2)\n",
    "\n",
    "# let's look at the first data point and Sigma_2\n",
    "r1 = compute_responsibility_v1(x_1, pis, mus, Sigmas_2)\n",
    "r2 = compute_responsibility_v2(x_1, pis, mus, Sigmas_2)\n",
    "print(r1, r2)\n",
    "\n",
    "# let's look at the second data point and Sigma_1\n",
    "r1 = compute_responsibility_v1(x_2, pis, mus, Sigmas_1)\n",
    "r2 = compute_responsibility_v2(x_2, pis, mus, Sigmas_1)\n",
    "print(r1, r2)\n",
    "\n",
    "# let's look at the second data point and Sigma_2\n",
    "r1 = compute_responsibility_v1(x_2, pis, mus, Sigmas_2)\n",
    "r2 = compute_responsibility_v2(x_2, pis, mus, Sigmas_2)\n",
    "print(r1, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
